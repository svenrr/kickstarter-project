{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training set\n",
    "Trainset = pd.read_csv('Kickstarter_Train.csv')\n",
    "Seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "\n",
    "Before we can start modelling we need to chose the features which will be used. Secondly we need to hot encode out 'category' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are only a few useful parameters we can pass to our prediction algorithms\n",
    "Trainset = Trainset[['blurb_length', 'category', 'converted_goal_amount',\n",
    "                     'name_length', 'days_total', 'days_until_launch', 'state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot-one-encoding of the category parameter\n",
    "category_dummies = pd.get_dummies(Trainset['category'], drop_first=True)\n",
    "\n",
    "# Put them together and drop the original category column\n",
    "Trainset = pd.concat([Trainset, category_dummies], axis=1)\n",
    "Trainset.drop('category', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "There is a lot of data, so another split into train and test can be performed.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the set into X and y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = Trainset.drop('state', axis=1)\n",
    "y = Trainset.state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "For scaling we use the RobustScaler as it is more sensitive towards outliers, which are present in the data.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Apply Scaling\n",
    "features = ['blurb_length', 'converted_goal_amount', 'name_length', 'days_total', 'days_until_launch']\n",
    "transformer = RobustScaler().fit(X[features]) # Fit the scaler on the complete Trainset. Later one we need to do the same with the validation set.\n",
    "X[features] = transformer.transform(X[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next up we import everything we need for the whole modelling process.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and other needed modules\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The focus is on recall, so a scorer is needed for later use in cross-validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scorer\n",
    "#scorer = make_scorer(recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "For evaluation we are going to use a few different parameters:    \n",
    "<ul>\n",
    "<li>Confusion matrix</li>\n",
    "<li>Classifciation report, which inclued recall, precision, f1-score etc.</li>\n",
    "<li>The singular recall score</li>\n",
    "<li>The Precision-Recall curve and the auc (area under curve)</li>\n",
    "</ul>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty df for the metrics we want to compare in the end\n",
    "df_metrics = pd.DataFrame({'model' : ['Dummy', 'KNN', 'DT', 'LR', 'RF'], 'recall' : [1,2,3,4,5], 'auc' : [1,2,3,4,5]})\n",
    "df_metrics.set_index('model', inplace=True)\n",
    "\n",
    "# Print out all evaluation metrics we want\n",
    "def evaluation_metrics (y, y_pred, y_proba, model='default'):\n",
    "    pos_probs = y_proba[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y, pos_probs)\n",
    "    recall_real = recall_score(y, y_pred).round(2)\n",
    "    auc_real = auc(recall, precision).round(2)\n",
    "    if model != 'default':\n",
    "        df_metrics.loc[model, 'recall'], df_metrics.loc[model, 'auc'] = recall_real, auc_real\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print('---------------------------------------------------------')\n",
    "    print(classification_report(y, y_pred))\n",
    "    print('---------------------------------------------------------')\n",
    "    print(f'Recall score: {recall_score(y, y_pred).round(2)}')\n",
    "    print('---------------------------------------------------------')\n",
    "    print(f'PR-AUC score: {auc(recall, precision).round(2)}')\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters from Grid-RandomizedSearch\n",
    "\n",
    "def best_parameters(model_name):\n",
    "    print('---------------')\n",
    "    print('Best parameters')\n",
    "    print('---------------')\n",
    "    for pair in model_name.best_params_.items():\n",
    "        print(f'{pair[0].capitalize()} : {pair[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "The dummy classifier can be used in multiple ways. One of the most common would be to simply let the classifier predict everything accoding to the most frequent class, which means it would in this case only predict positives, since 58% of all outcomes are positive. This would lead to a recall score of 1, as there are no false negatives.\n",
    "This is not what we want as baseline, since our goal is to achieve a high recall with a good area under curve score. The baseline would not offer good comparables.\n",
    "Therefore we chose to use another approach and let the classifier chose the outcome somewhat randomly, while still keeping the imbalance of the two classes in mind.\n",
    "This actually gives us a baseline which we can improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[19570 27576]\n",
      " [27580 38494]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42     47146\n",
      "           1       0.58      0.58      0.58     66074\n",
      "\n",
      "    accuracy                           0.51    113220\n",
      "   macro avg       0.50      0.50      0.50    113220\n",
      "weighted avg       0.51      0.51      0.51    113220\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.58\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.7\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=Seed) # 'stratified' generates predictions by respecting the training setâ€™s class distribution.\n",
    "\n",
    "# Fit on the data\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target value\n",
    "y_pred_dummy = dummy_clf.predict(X_train)\n",
    "\n",
    "# Calculate probability and precision/recall-auc-score\n",
    "y_proba_dummy = dummy_clf.predict_proba(X_train)\n",
    "\n",
    "evaluation_metrics(y_train, y_pred_dummy, y_proba_dummy, 'Dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Our baseline scores are 0.58 for Recall and 0.7 for AUC.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Best parameters\n",
      "---------------\n",
      "N_neighbors : 35\n",
      "Metric : manhattan\n"
     ]
    }
   ],
   "source": [
    "# Define model and parameters for RandomizedSearchCV\n",
    "knn = KNeighborsClassifier()\n",
    "param_dist_knn = {'n_neighbors' : np.linspace(5, 100, num = 20, dtype='int64').tolist(),\n",
    "                  'metric' : ['manhattan', 'euclidean']\n",
    "                 }\n",
    "\n",
    "# RandomizedSearchCV\n",
    "knn_search = RandomizedSearchCV(knn, param_dist_knn, verbose=1, n_jobs=-1, random_state=Seed, cv=5)\n",
    "knn_search.fit(X_train, y_train)\n",
    "\n",
    "# Print bester parameters\n",
    "best_parameters(knn_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[23065 24081]\n",
      " [10096 55978]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.49      0.57     47146\n",
      "           1       0.70      0.85      0.77     66074\n",
      "\n",
      "    accuracy                           0.70    113220\n",
      "   macro avg       0.70      0.67      0.67    113220\n",
      "weighted avg       0.70      0.70      0.69    113220\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.85\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.78\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters \n",
    "knn_best = knn_search.best_estimator_\n",
    "\n",
    "# Fit on the traindata\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target value\n",
    "y_pred_knn = knn_best.predict(X_train)\n",
    "\n",
    "y_proba_knn = knn_best.predict_proba(X_train)\n",
    "\n",
    "evaluation_metrics (y_train, y_pred_knn, y_proba_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "A Recall score of 0.85, while maintaining a AUC of 0.78 looks promising. Remember though, that this is done only with the trainingset.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[ 7307  8408]\n",
      " [ 3691 18334]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.46      0.55     15715\n",
      "           1       0.69      0.83      0.75     22025\n",
      "\n",
      "    accuracy                           0.68     37740\n",
      "   macro avg       0.67      0.65      0.65     37740\n",
      "weighted avg       0.68      0.68      0.67     37740\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.83\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.74\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict the Testset\n",
    "y_pred_knn_test = knn_best.predict(X_test)\n",
    "\n",
    "y_proba_knn_test = knn_best.predict_proba(X_test)\n",
    "\n",
    "evaluation_metrics (y_test, y_pred_knn_test, y_proba_knn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Recall is still good. The AUC gets close to the baseline of 0.7 though.\n",
    "We have to look at the validation set later to see how it holds up.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   34.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Best parameters\n",
      "---------------\n",
      "Splitter : best\n",
      "Min_samples_split : 5\n",
      "Max_leaf_nodes : 100\n",
      "Max_features : None\n",
      "Max_depth : 90\n",
      "Criterion : gini\n"
     ]
    }
   ],
   "source": [
    "# Define model and parameters for GridSearch\n",
    "tree = DecisionTreeClassifier(random_state=Seed, class_weight='balanced')\n",
    "param_dist_tree = {'criterion' : ['gini', 'entropy'],\n",
    "                   'splitter' : ['best', 'random'],\n",
    "                   'max_depth' : np.linspace(5, 100, num = 20, dtype='int64').tolist(),\n",
    "                   'max_features' : ['sqrt', 'log2', None],\n",
    "                   'min_samples_split' : np.linspace(5, 20, num = 5, dtype='int64').tolist(),\n",
    "                   'max_leaf_nodes' : np.linspace(1, 100, num = 50, dtype='int64').tolist() + [None]\n",
    "                  }\n",
    "\n",
    "# Run GridSearch\n",
    "tree_search = RandomizedSearchCV(tree, param_dist_tree, cv=5, n_jobs=-1, verbose=1, n_iter=100)\n",
    "tree_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "best_parameters(tree_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[30375 16771]\n",
      " [17792 48282]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.64      0.64     47146\n",
      "           1       0.74      0.73      0.74     66074\n",
      "\n",
      "    accuracy                           0.69    113220\n",
      "   macro avg       0.69      0.69      0.69    113220\n",
      "weighted avg       0.70      0.69      0.70    113220\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.73\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.79\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters on the model\n",
    "tree_best = tree_search.best_estimator_\n",
    "\n",
    "# Fit on the data\n",
    "tree_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target value\n",
    "y_pred_tree = tree_best.predict(X_train)\n",
    "y_proba_tree = tree_best.predict_proba(X_train)\n",
    "\n",
    "evaluation_metrics(y_train, y_pred_tree, y_proba_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The Recall score on the trainset is 0.77, which is worse than KNN.  \n",
    "The Curve score is better with 0.79.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[10197  5518]\n",
      " [ 5973 16052]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64     15715\n",
      "           1       0.74      0.73      0.74     22025\n",
      "\n",
      "    accuracy                           0.70     37740\n",
      "   macro avg       0.69      0.69      0.69     37740\n",
      "weighted avg       0.70      0.70      0.70     37740\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.73\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.78\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testset\n",
    "y_pred_tree_test = tree_best.predict(X_test)\n",
    "y_proba_tree_test = tree_best.predict_proba(X_test)\n",
    "\n",
    "evaluation_metrics(y_test, y_pred_tree_test, y_proba_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Recall and Curve score of the testset are nearly identical to the trainset, which at least speaks for the models consistency.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Best parameters\n",
      "---------------\n",
      "C : 0.05\n"
     ]
    }
   ],
   "source": [
    "# Define model and parameters for GridSearch\n",
    "lr = LogisticRegression(random_state=Seed, class_weight='balanced')\n",
    "param_dist_lr = {'C' : [0.01, 0.025, 0.05, 0.1, 0.5, 1, 2]\n",
    "                }\n",
    "\n",
    "# Run GridSearch\n",
    "lr_search = GridSearchCV(lr, param_dist_lr, cv=5, n_jobs=-1, verbose=1)\n",
    "lr_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "best_parameters(lr_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[29852 17294]\n",
      " [23760 42314]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59     47146\n",
      "           1       0.71      0.64      0.67     66074\n",
      "\n",
      "    accuracy                           0.64    113220\n",
      "   macro avg       0.63      0.64      0.63    113220\n",
      "weighted avg       0.65      0.64      0.64    113220\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.64\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.72\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters on the model\n",
    "lr_best = lr_search.best_estimator_\n",
    "\n",
    "# Fit on the data\n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target value\n",
    "y_pred_lr = lr_best.predict(X_train)\n",
    "y_proba_lr = lr_best.predict_proba(X_train)\n",
    "\n",
    "evaluation_metrics(y_train, y_pred_lr, y_proba_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Recall 0.64 and AUC 0.72.\n",
    "Both are only slightly better than to chose the outcome at random.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[10015  5700]\n",
      " [ 7802 14223]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60     15715\n",
      "           1       0.71      0.65      0.68     22025\n",
      "\n",
      "    accuracy                           0.64     37740\n",
      "   macro avg       0.64      0.64      0.64     37740\n",
      "weighted avg       0.65      0.64      0.64     37740\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.65\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.73\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testset\n",
    "y_pred_lr_test = lr_best.predict(X_test)\n",
    "y_proba_lr_test = lr_best.predict_proba(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "evaluation_metrics(y_test, y_pred_lr_test, y_proba_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The scores stay pretty much the same. We'd have to do some further analysis to find out if we could improve upon that.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Best parameters\n",
      "---------------\n",
      "Min_samples_split : 15\n",
      "Min_samples_leaf : 10\n",
      "Max_leaf_nodes : 100\n",
      "Max_features : sqrt\n",
      "Max_depth : 20.0\n",
      "Class_weight : balanced_subsample\n",
      "Bootstrap : True\n"
     ]
    }
   ],
   "source": [
    "# Define model and parameters for RandomizedSearchCV\n",
    "rf = RandomForestClassifier(random_state=Seed, class_weight='balanced')\n",
    "param_dist_rf = {'max_features' : ['sqrt', 'log2'],\n",
    "                 'max_depth' : np.linspace(10, 110, num = 11).tolist() + [None],\n",
    "                 'min_samples_split' : [5, 10, 15],\n",
    "                 'min_samples_leaf' : [10,15,20],\n",
    "                 'bootstrap' : [True, False],\n",
    "                 'max_leaf_nodes' : [50,100],\n",
    "                 'class_weight' : ['balanced', 'balanced_subsample']\n",
    "                }\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "rf_search = RandomizedSearchCV(rf, param_dist_rf, cv = 5, n_iter=100,\n",
    "                               verbose=1, random_state=Seed, n_jobs = -1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "best_parameters(rf_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[31370 15776]\n",
      " [18226 47848]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     47146\n",
      "           1       0.75      0.72      0.74     66074\n",
      "\n",
      "    accuracy                           0.70    113220\n",
      "   macro avg       0.69      0.69      0.69    113220\n",
      "weighted avg       0.70      0.70      0.70    113220\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.72\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.8\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters on the model\n",
    "rf_best = rf_search.best_estimator_\n",
    "\n",
    "# Fit on the data\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target value\n",
    "y_pred_rf = rf_best.predict(X_train)\n",
    "y_proba_rf = rf_best.predict_proba(X_train)\n",
    "\n",
    "# Print evaluation metrics\n",
    "evaluation_metrics(y_train, y_pred_rf, y_proba_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The recall score is quite low with 0.72, but we have the highest curve score yet, which suggests a good balance between precision and recall.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[10494  5221]\n",
      " [ 6096 15929]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     15715\n",
      "           1       0.75      0.72      0.74     22025\n",
      "\n",
      "    accuracy                           0.70     37740\n",
      "   macro avg       0.69      0.70      0.69     37740\n",
      "weighted avg       0.70      0.70      0.70     37740\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.72\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.8\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict testset\n",
    "y_pred_rf_test = rf_best.predict(X_test)\n",
    "y_proba_rf_test = rf_best.predict_proba(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "evaluation_metrics(y_test, y_pred_rf_test, y_proba_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Pretty much the same as the trainset.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation on unused data\n",
    "\n",
    "We now important the validation set and transform it the same way as the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_data = pd.read_csv('Kickstarter_Validation.csv')\n",
    "Val_data = Val_data[['blurb_length', 'category', 'converted_goal_amount',\n",
    "            'name_length', 'days_total', 'days_until_launch', 'state']]\n",
    "\n",
    "# Hot-one-encoding of the category parameter\n",
    "category_dummies = pd.get_dummies(Val_data['category'], drop_first=True)\n",
    "\n",
    "# Put them together and drop the original category column\n",
    "Val_data = pd.concat([Val_data, category_dummies], axis=1)\n",
    "Val_data.drop('category', axis=1, inplace=True)\n",
    "\n",
    "# Split the set into X and y\n",
    "X_val = Val_data.drop('state', axis=1)\n",
    "y_val = Val_data.state\n",
    "\n",
    "# Apply scaling from trainset\n",
    "features = ['blurb_length', 'converted_goal_amount', 'name_length', 'days_total', 'days_until_launch']\n",
    "X_val[features] = transformer.transform(X_val[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[    7 20947]\n",
      " [    0 29366]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00     20954\n",
      "           1       0.58      1.00      0.74     29366\n",
      "\n",
      "    accuracy                           0.58     50320\n",
      "   macro avg       0.79      0.50      0.37     50320\n",
      "weighted avg       0.76      0.58      0.43     50320\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 1.0\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.59\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict the target value\n",
    "y_pred_knn_val = knn_best.predict(X_val)\n",
    "y_proba_knn_val = knn_best.predict_proba(X_val)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "evaluation_metrics(y_val, y_pred_knn_val, y_proba_knn_val, 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "KNN very obviously has a problem here. The Recall score is at 1 because the algorithm set nearly all predictions as positive. The curve score dropped below 0.6, which is worse than the baseline.  \n",
    "It is clear that the KNN in this iteration doesn't seem to be an appropriate algorithm for the problem at hand.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[  488 20466]\n",
      " [  359 29007]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.02      0.04     20954\n",
      "           1       0.59      0.99      0.74     29366\n",
      "\n",
      "    accuracy                           0.59     50320\n",
      "   macro avg       0.58      0.51      0.39     50320\n",
      "weighted avg       0.58      0.59      0.45     50320\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.99\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.68\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict the target value\n",
    "y_pred_tree_val = tree_best.predict(X_val)\n",
    "y_proba_tree_val = tree_best.predict_proba(X_val)\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "evaluation_metrics(y_val, y_pred_tree_val, y_proba_tree_val, 'DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Just like KNN, the decision tree predicts nearly all outcomes as positive, which gives it a high recall, at the price of precision.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[ 7314 13640]\n",
      " [ 4337 25029]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.35      0.45     20954\n",
      "           1       0.65      0.85      0.74     29366\n",
      "\n",
      "    accuracy                           0.64     50320\n",
      "   macro avg       0.64      0.60      0.59     50320\n",
      "weighted avg       0.64      0.64      0.62     50320\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.85\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.69\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict the target value\n",
    "y_pred_lr_val = lr_best.predict(X_val)\n",
    "y_proba_lr_val = lr_best.predict_proba(X_val)\n",
    "\n",
    "# Print evaluation metrics\n",
    "evaluation_metrics(y_val, y_pred_lr_val, y_proba_lr_val, 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The logistic regression does far better than expected, witch good recall and an ok AUC.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "[[ 8719 12235]\n",
      " [ 7256 22110]]\n",
      "---------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.47     20954\n",
      "           1       0.64      0.75      0.69     29366\n",
      "\n",
      "    accuracy                           0.61     50320\n",
      "   macro avg       0.59      0.58      0.58     50320\n",
      "weighted avg       0.60      0.61      0.60     50320\n",
      "\n",
      "---------------------------------------------------------\n",
      "Recall score: 0.75\n",
      "---------------------------------------------------------\n",
      "PR-AUC score: 0.69\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict the target value\n",
    "y_pred_rf_val = rf_best.predict(X_val)\n",
    "y_proba_rf_val = rf_best.predict_proba(X_val)\n",
    "\n",
    "# print evaluation metrics\n",
    "evaluation_metrics(y_val, y_pred_rf_val, y_proba_rf_val, 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The Random forest does really well with a recall score of 0.87 and a curve score of 0.7. This suggests a suiteable tradeoff between precision and recall for our purposes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
